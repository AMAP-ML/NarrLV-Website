<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <meta name="description"
        content="NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models">
  <meta name="keywords" content="Evaluation, Benchmark, Prompts, Dataset, Video Generation, Text-to-Video, Stable Diffusion, AIGC, Image Generation, AMAP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models</title>

<!--   Global site tag (gtag.js) - Google Analytics-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5ZVQZ7NHC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-Y5ZVQZ7NHC');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">

  <link rel="stylesheet" href="assets/narrlv/css/bulma.min.css">
  <link rel="stylesheet" href="assets/narrlv/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/narrlv/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/narrlv/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/narrlv/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="assets/narrlv/js/fontawesome.all.min.js"></script>
  <script src="assets/narrlv/js/bulma-carousel.min.js"></script>
  <script src="assets/narrlv/js/bulma-slider.min.js"></script>
  <script src="assets/narrlv/js/index.js"></script>
</head>
<body>

<!-- title -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
<!--          <h1 class="title is-1 publication-title"><span style="color:#B9770E; font-weight: bold; font-style: italic">NarrLV</span> : Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models</h1>-->
            <h1 class="title is-1 publication-title">NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">

              <span class="author-block">
                  Xiaokun Feng<sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                  Haiming Yu<sup>3</sup>,
              </span>
              <span class="author-block">
                  Meiqi Wu<sup>3,4*</sup>,
              </span>
              <span class="author-block">
                  Shiyu Hu<sup>5</sup>,
              </span>
              <span class="author-block">
                  Jintao Chen<sup>3,6</sup>,
              </span>
                <br>
              <span class="author-block">
                  Chen Zhu<sup>3,</sup>,
              </span>
              <span class="author-block">
                  Jiahong Wu<sup>3</sup>,
              </span>
              <span class="author-block">
                  Xiangxiang Chu<sup>3</sup>,
              </span>
              <span class="author-block">
                  Kaiqi Huang<sup>1,2</sup>,
              </span>
              <br>
            </span>
          </div>
          <!-- <br> -->

        <div class="is-size-5 publication-authors">
          <span class="author-block">
            <sup>1</sup>
              School of Artificial Intelligence, UCAS &nbsp;&nbsp;
            <sup>2</sup>
              CASIA &nbsp;&nbsp;
            <sup>3</sup>
              AMAP, Alibaba Group &nbsp;&nbsp;
            <br>
            <sup>4</sup>
              School of Computer Science and Technology, UCAS &nbsp;&nbsp;
            <br>
            <sup>5</sup>
              School of Physical and Mathematical Sciences, NTU  &nbsp;&nbsp;
            <sup>6</sup>
              PKU &nbsp;&nbsp;
          </span>
        </div>

        <div class="column has-text-centered">
          <div class="publication-links">
            
            <span class="link-block">
              <a href="https://arxiv.org/abs/2507.11245" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/AMAP-ML/NarrLV" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>GitHub</span>
              </a>
            </span>
            <!-- Huggingface Demo Link. -->
            <span class="link-block">
              <a href="https://huggingface.co/datasets/Xiaokunfeng2022/NarrLV" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="assets/narrlv/hf-logo.svg" style="display:block;width:330px;height:240px" />
                </span>
                <span>Huggingface</span>
              </a>
            </span>
          </div>

        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract. -->
<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3 is-centered">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to greater content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. 
However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (\eg, VBench).
To comprehensively assess the <b>Narr</b>ative expression capabilities of <b>L</b>ong <b>V</b>ideo generation models, we propose <b>NarrLV</b> - a novel benchmark inspired by film narrative theory.
(<b>i</b>) First, we introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs.
(<b>ii</b>) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework.
(<b>iii</b>) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models that underpin them. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: left;"><b>Framework of our NarrLV</b></h2>
        <div class="content">
          <div class="hero-body" style="padding: 0;">
            <p style="margin-top: 0; white-space: pre-line;">
              (a) Our prompt suite is inspired by film narrative theory and identifies three key factors influencing Temporal Narrative Atom (TNA) transitions. Based on these, we construct a prompt generation pipeline capable of producing evaluation prompts with flexibly adjustable TNA counts.
              
              (b) Our evaluation models include long video generation models and the foundation models they often rely on.
              
              (c) Based on the progressive expression of narrative content, we conduct evaluations from three dimensions, employing an MLLM-based question generation and answering framework for calculations. Our metric is well-aligned with human preferences.
            </p>
            <img src="assets/narrlv/images/fig_framework.jpg" style="width:100%; margin-bottom:2px" alt="Teaser."/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: left;"><b>Evaluation Results</b></h2>
        <div class="content">
          <div class="hero-body" style="padding: 0;">
            <p style="margin-top: 0; white-space: pre-line;">
              Our evaluation model encompasses existing long video generation models as well as the foundational generation models they typically rely on: 
            </p>
            <img src="assets/narrlv/images/eval1.png" style="width:100%; margin-bottom:2px" alt="Teaser."/>
            <img src="assets/narrlv/images/eval2.png" style="width:100%; margin-bottom:2px" alt="Teaser."/>

            <p style="margin-top: 0; white-space: pre-line;">
              <b>Note</b>: We will continuously update the evaluation results of the latest long video generation models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: left;"><b>Extensible TNA-Driven Prompt Suite</b></h2>
        <div class="content">
          <div class="hero-body" style="padding: 0;">
            <p style="margin-top: 0; white-space: pre-line;">
              The number of Temporal Narrative Atoms (TNA) serves as a quantitative measure of narrative richness. Unlike existing representative benchmarks that concentrate on prompts with only a small number of TNAs in a narrow range, our innovative prompt suite can flexibly expand narrative content richness, thereby enabling a thorough assessment of the full narrative capabilities of long video generation models. Its word cloud clearly shows that words like "suddenly," "next," and "finally," which pertain to the progression of narrative content, hold significant weight, aligning with our narrative-centric evaluation objectives.
            </p>
            <img src="assets/narrlv/images/readme_prompt_suit.png" style="width:100%; margin-bottom:2px" alt="Teaser."/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: left;"><b>Progressive Narrative-Expressive Evaluation Metric</b></h2>
        <div class="content">
          <div class="hero-body" style="padding: 0;">
            <p style="margin-top: 0; white-space: pre-line;">
              To systematically evaluate the narrative quality of long video generation, we introduce three core metrics: Narrative Element Fidelity (<b><i>R</i></b><sub>fid</sub>), Narrative Unit Coverage (<b><i>R</i></b><sub>cov</sub>), and Narrative Unit Coherence (<b><i>R</i></b><sub>coh</sub>). Among these, <b><i>R</i></b><sub>fid</sub> focuses on the generation performance of narrative elements represented by scenes and objects. <b><i>R</i></b><sub>cov</sub> and <b><i>R</i></b><sub>coh</sub> emphasize the generation quality of narrative units composed of narrative elements. In addition, we utilize a recently popular MLLM-based question generation and answer framework for quantitative evaluation.
            </p>
            <img src="assets/narrlv/images/readme_prompt_suit.png" style="width:100%; margin-bottom:2px" alt="Teaser."/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/AMAP-ML/NarrLV" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>


